{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2470db6",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6989df65",
   "metadata": {},
   "source": [
    "# Training compass: why $\\rightarrow$ what $\\rightarrow$ how\n",
    "- Why: the question nobody wants to answer\n",
    "- What: translating goals into decision\n",
    "- Super Power: speed and data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1157db22",
   "metadata": {},
   "source": [
    "# Every big models starts with a small ablation\n",
    "- Choosing your basline\n",
    "- Picking a training framework\n",
    "    - Megatron-LLM from Nvidia\n",
    "    - DeepSpeed from Micro\n",
    "    - TorchTitan from torch\n",
    "    - Nanotron from hugging face\n",
    "- Ablation setup\n",
    "    - Setting up our ablation framework  \n",
    "        - the goal of ablations is to run experiments at a small scale and get results we can confidently extrapolate to our final production run.\n",
    "        - two main approaches\n",
    "            - target model size + fewer tokens\n",
    "            - small proxy model\n",
    "        - paramenters\n",
    "            - Embeddings(input + outpur Projection): vocab_size * hidden_size\n",
    "                - every token in the vocab need an embedding vector\n",
    "            - Attention Layers: Layers * $hidden\\_size^{2}$ * (2+2*kv_heads/num_heads)\n",
    "                - Q: projection $hidden\\_size^{2}$\n",
    "                - K and V: kv_heads/number_heads * $hidden\\_size^{2}$\n",
    "                - output: $hidden\\_size^{2}$\n",
    "            - Feed forward: layers * hidden_size * intermedia_size * 3\n",
    "                - up: hidden_size * intermediate_size\n",
    "                - gate: hidden_size * intermedia_size\n",
    "                - low: intermedia_size * hidden_size\n",
    "            - Layer Norms: layers * hidden_size * 2\n",
    "                - LayerNorms(x) = $\\gamma\\cdot\\frac{x-\\mu}{\\sqrt{\\delta^{2}+\\epsilon}}+\\beta$\n",
    "                - for the same layer the $\\gamma$ and $\\beta$ are the same\n",
    "                - $\\gamma$ and $\\beta$ are hidden_size vector for each input\n",
    "                - the different columns of each input multiply different elements of $\\gamma$ and $\\beta$\n",
    "    - Understanding what works: evaluation\n",
    "        - Monotonicity\n",
    "        - Low noise\n",
    "        - Above-random performance\n",
    "        - Ranking consistencey\n",
    "    - Estimating ablations cost\n",
    "- Rules of engagement\n",
    "    - Be paranoid\n",
    "    - Validate your evaluation suite\n",
    "    - Test every change, no matter how small\n",
    "    - Change one thing at a time\n",
    "    - Train on enough tokens and use sufficient evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47308087",
   "metadata": {},
   "source": [
    "# Designing the model architecture\n",
    "- Architecture choices\n",
    "    - Attention\n",
    "    - Embedding sharing\n",
    "    - Positional Encodings & long Context\n",
    "    - Improving stability\n",
    "    - Other core components\n",
    "    - Going Sparse: MoE\n",
    "    - Excursion: Hybrid Models\n",
    "    - To MoE or not MoE: Choosing a Base Architecture\n",
    "    - The tokenizer\n",
    "    - SmolLM3\n",
    "    - Rules of engagement\n",
    "- Optimiser and training hyperparameters\n",
    "- Scaling laws: how many parameters, how much data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f7b9fd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
